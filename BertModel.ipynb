{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BertModel.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyOiNCRYn1UDYiquzQQtiwea"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"bab800fffe004260a2d3e2e87773a43f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_53f650fd58324d6287d97224ff73e668","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_8dd84a79da144872bdbe42e0acb9b146","IPY_MODEL_e2139fcfb31e427b9affb036096ccfa4"]}},"53f650fd58324d6287d97224ff73e668":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8dd84a79da144872bdbe42e0acb9b146":{"model_module":"@jupyter-widgets/controls","model_name":"IntProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_919ef4fb7d744a478adbb03fbf224b7f","_dom_classes":[],"description":"Downloading: 100%","_model_name":"IntProgressModel","bar_style":"success","max":434,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":434,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_3497d474cf5c490da3610352fe78020b"}},"e2139fcfb31e427b9affb036096ccfa4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_46987af1ef064057871f860b5a2472ec","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 434/434 [00:00&lt;00:00, 1.35kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_90be704265fb4bf398ce6319901f204a"}},"919ef4fb7d744a478adbb03fbf224b7f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"3497d474cf5c490da3610352fe78020b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"46987af1ef064057871f860b5a2472ec":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"90be704265fb4bf398ce6319901f204a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0bd0507275c84d3f8cc792dacce15ba1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_3827fb6bd3224decb66d97cabd5def32","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_d296aefdff4e4be4bc65d5ce5f942d9c","IPY_MODEL_32fc7e14ec91457fba0842c80c9ad705"]}},"3827fb6bd3224decb66d97cabd5def32":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d296aefdff4e4be4bc65d5ce5f942d9c":{"model_module":"@jupyter-widgets/controls","model_name":"IntProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_e515e6e155cc41c58fcb75b7e9e72ca9","_dom_classes":[],"description":"Downloading: 100%","_model_name":"IntProgressModel","bar_style":"success","max":1344997306,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1344997306,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c8490892ef5d4c1eb930a59b2588ef06"}},"32fc7e14ec91457fba0842c80c9ad705":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_6322b6a0cc2e4c9fa7cd7ec99169bd37","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1.34G/1.34G [00:46&lt;00:00, 28.9MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_61191fd91633422ca516f94325f59818"}},"e515e6e155cc41c58fcb75b7e9e72ca9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"c8490892ef5d4c1eb930a59b2588ef06":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6322b6a0cc2e4c9fa7cd7ec99169bd37":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"61191fd91633422ca516f94325f59818":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"29f9159510454568aebe8f65ff815e6a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_5c668e33f9884e2dbc92b3441c1378a6","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_dbe06afa99a340319bf353a7cd107a56","IPY_MODEL_250f15549ded495cbc8c43638fa0d0c1"]}},"5c668e33f9884e2dbc92b3441c1378a6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"dbe06afa99a340319bf353a7cd107a56":{"model_module":"@jupyter-widgets/controls","model_name":"IntProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_ccda3df2e5f6493b9a97135df5e9236c","_dom_classes":[],"description":"Downloading: 100%","_model_name":"IntProgressModel","bar_style":"success","max":231508,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":231508,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_5c4da7f4ef8f404cb96a5787e6a73efc"}},"250f15549ded495cbc8c43638fa0d0c1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_bf6ee12da564499e9d4f5dcffb62960d","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 232k/232k [00:00&lt;00:00, 2.66MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_cedac71061294662aabf8b4ca5d23be8"}},"ccda3df2e5f6493b9a97135df5e9236c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"5c4da7f4ef8f404cb96a5787e6a73efc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"bf6ee12da564499e9d4f5dcffb62960d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"cedac71061294662aabf8b4ca5d23be8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"V5dJBhaieeyU","colab_type":"text"},"source":["#Now to implement our own Bert architecture"]},{"cell_type":"code","metadata":{"id":"b2UEvzTjfFN2","colab_type":"code","colab":{}},"source":["%%capture\n","!pip install bert-extractive-summarizer\n","!pip install spacy\n","!pip install transformers==2.2.2\n","!pip install neuralcoref\n","from transformers import *"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"X3ZsrEEfeU0J","colab_type":"code","outputId":"162c43eb-f0cc-42ba-8496-b22ed26e679e","executionInfo":{"status":"ok","timestamp":1587835450956,"user_tz":-330,"elapsed":90147,"user":{"displayName":"Edgar Monis","photoUrl":"","userId":"18244376460613966063"}},"colab":{"base_uri":"https://localhost:8080/","height":115,"referenced_widgets":["bab800fffe004260a2d3e2e87773a43f","53f650fd58324d6287d97224ff73e668","8dd84a79da144872bdbe42e0acb9b146","e2139fcfb31e427b9affb036096ccfa4","919ef4fb7d744a478adbb03fbf224b7f","3497d474cf5c490da3610352fe78020b","46987af1ef064057871f860b5a2472ec","90be704265fb4bf398ce6319901f204a","0bd0507275c84d3f8cc792dacce15ba1","3827fb6bd3224decb66d97cabd5def32","d296aefdff4e4be4bc65d5ce5f942d9c","32fc7e14ec91457fba0842c80c9ad705","e515e6e155cc41c58fcb75b7e9e72ca9","c8490892ef5d4c1eb930a59b2588ef06","6322b6a0cc2e4c9fa7cd7ec99169bd37","61191fd91633422ca516f94325f59818"]}},"source":["model_to_emulate = BertModel.from_pretrained('bert-large-uncased', output_hidden_states=True).to('cuda:0').eval()\n","\n","game_of_thrones = '''\n","A Game of Thrones takes place over the course of one year on or near the fictional continent of Westeros. The story begins when King Robert visits the northern castle Winterfell to ask Ned Stark to be his right-hand assistant, or Hand of the King. The previous Hand, Jon Arryn, died under suspicious circumstances. Robert comes with his queen, Cersei Lannister, and his retinue, which includes a number of Lannisters. Just after the royal party arrives, Ned’s wife, Catelyn, receives a message claiming that the Lannister family was responsible for the death of the former Hand. She tells Ned, who accepts the position as Hand in order to protect Robert from the Lannisters. Ned’s son Bran then discovers Cersei Lannister and her brother Jaime Lannister having sex, and Jaime pushes Bran from a window to silence him. Everyone thinks Bran simply fell while climbing around the castle. While Bran is still unconscious, Ned leaves Winterfell and rides south with Robert. The same day, Ned’s bastard son, Jon, leaves to serve at the Wall, a massive structure that protects Westeros from the wilderness of the far North. The group of men sworn to defend the Wall, the Night’s Watch, have been receiving reports of strange creatures and have been losing men with increasing frequency. Tyrion Lannister, a little person who is brother to Cersei and Jaime, travels with Jon to the Wall to see the massive structure. Meanwhile, on a continent east of Westeros, Daenerys Targaryen marries the warlord Khal Drogo, one of the leaders of the Dothraki people. Daenerys and her brother Viserys are the last surviving members of the family Robert defeated to become king, the Targaryens. They are an old family said to be descended from dragons, and Viserys thinks with Khal Drogo’s army he can retake the throne. A knight named Ser Jorah Mormont, exiled by Ned Stark, pledges he will help. Daenerys receives three dragon eggs as a wedding gift and becomes immediately fascinated by them.\n","On the trip south to the capital, called King’s Landing, Robert’s and Cersei’s son, Joffrey, and Ned’s daughter Sansa, who everyone presumes will be married one day, go for a walk. When Joffrey sees Arya, Ned’s other daughter and sister to Sansa, practicing her swordfighting with a boy, he decides to show them he’s a better fighter. As pets, all of the Stark children have direwolf pups, a wolf breed larger than normal wolves that also happens to be the symbol of the Stark house, and Arya’s wolf injures Joffrey defending her. Though Sansa knows Joffrey instigated the fight, she will not tell on Joffrey because she’s in love with him. As punishment, Cersei wants Arya’s wolf killed, but since it ran away after hurting Joffrey, Cersei demands that Ned kill Sansa’s wolf instead. Meanwhile, an assassin tries to kill the unconscious Bran and fails. Ned finally reaches King’s Landing to find that Catelyn has sailed to the city in secret to discover the truth about the assassin. She has the dagger the assassin used, and after examining it, Catelyn’s childhood friend Littlefinger recognizes it as belonging to Tyrion Lannister. Ned tells Catelyn he will try to determine who killed the former Hand, Jon Arryn, and tried to kill Bran. Bran finally wakes from his coma, but he doesn’t remember how he fell. Tyrion visits him on his way south from the Wall to deliver a greeting from Jon. Tyrion continues south as Catelyn starts back north, and when their paths cross Catelyn has him seized for trying to kill Bran.\n","In King’s Landing, Ned slowly begins to unravel the mystery of why the previous Hand was killed. He knows it has to do with something the Hand learned about King Robert’s children. Through a spy, Robert learns that Daenerys Targaryen is pregnant. He wants to assassinate her because he fears she and her son will one day challenge Robert’s right to the throne. Disgusted with Robert’s plan, Ned resigns as Hand. That night, Jaime and his men confront Ned about Tyrion’s capture. Jaime kills Ned’s men and Ned breaks his leg while fighting. The following day, Robert reinstates Ned as hand. While Robert is gone hunting, Ned orders the execution of a rogue knight loyal to the Lannister family who has been pillaging villages. Further north, Catelyn takes Tyrion to her sister Lysa Arryn’s castle, the Eyrie, which is in a mountainous area called The Vale. Lysa accuses Tyrion of arranging the murder of both Jon Arryn and Bran. Tyrion denies the accusations and demands a trial by combat. A knight fights on Lysa’s behalf, and a mercenary fights on Tyrion’s behalf. Tyrion’s mercenary wins. While Tyrion rides from the Eyrie, a group of mountain clansmen try to kill him, but he promises to help them take The Vale and convinces them to join him.\n","In the east, as Khal Drogo and his Dothraki followers head back to Vaes Dothrak, their capital, Viserys becomes increasingly angry that Drogo has not provided him with an army with which to wage war on Westeros. During a feast he attacks Daenerys in a rage, and Khal Drogo has Viserys killed by dumping molten gold on his head. In King’s Landing, Ned has figured out why the Hand was killed: he had discovered that Joffrey was not really Robert’s child but was actually the product of Cersei’s sexual relationship with her brother, Jaime. Robert doesn’t know the truth. Robert is mortally wounded in a hunt, and before he dies, he names Ned the Protector of the Realm, essentially an interim king, until Joffrey comes of age. Ned does not tell Robert that he knows Joffrey is not the true heir, since he is the son of Cersei and Jaime. Ned asks Littlefinger’s help to install the true heir, Robert’s brother Stannis, as the king, and Littlefinger agrees. But when Ned confronts the Lannisters, saying that Joffrey is not the true heir and expecting Littlefinger’s support, Littlefinger betrays him, and Cersei imprisons Ned for treason. Meanwhile, north of the Wall, Jon and other men have discovered two strange dead bodies. They bring the bodies back for examination, and late at night, one of the bodies comes to life and tries to kill Jon’s commander. Jon and his direwolf fight off the undead body, and Jon kills the creature with fire.\n","After Ned’s capture, Arya escapes the castle in King’s Landing and Cersei holds Sansa hostage (she says she is holding Sansa for her own protection). Tywin Lannister, father to Tyrion, Cersei, and Jaime, wages war with Catelyn and her son, Robb Stark. Shrewdly outmaneuvering Tywin, Robb manages to defeat a portion of the Lannister army and capture Jaime. In King’s Landing, Joffrey, who is considered Robert’s heir, is crowned King. In the hopes that he can prevent his daughters being harmed, Ned confesses publicly to treason, and Joffrey has him executed while Sansa watches. Arya is in the crowd, though nobody knows it. Learning of his father’s death and his brother’s march to war, Jon tries to desert the Wall. But Jon’s friends convince him that he must stay and defend the Wall as he vowed. In the east, Drogo suffers a wound while raiding a village. Daenerys has one of the village women treat him, but he becomes very sick. When he is about to die, Daenerys asks the woman to heal him, but she says only bloodmagic will save him and it will require a death in exchange for his life. Despite the protests of the Dothraki people, Daenerys has the woman do it. Shortly thereafter Daenerys goes into labor. When she wakes many days later, her child is dead and Drogo is alive but brain dead. His followers have all left, leaving behind only a few warriors, the sick and old, and Ser Jorah Mormont, who has become one of Daenerys’s most trusted advisors. Daenerys sets a funeral pyre to burn Drogo and the woman, who tricked her and essentially killed her husband and child. Daenerys also places the three dragon eggs into the pyre. As the fire burns, Daenerys walks into it, and when finally it clears, Mormont and the Dothrakis find her with three newborn dragons at her breast.\n","'''"],"execution_count":2,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bab800fffe004260a2d3e2e87773a43f","version_minor":0,"version_major":2},"text/plain":["HBox(children=(IntProgress(value=0, description='Downloading', max=434, style=ProgressStyle(description_width=…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0bd0507275c84d3f8cc792dacce15ba1","version_minor":0,"version_major":2},"text/plain":["HBox(children=(IntProgress(value=0, description='Downloading', max=1344997306, style=ProgressStyle(description…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"igw1Srb-jE7n","colab_type":"code","outputId":"08083871-8d25-4a2f-d3c8-c020bf6ad7d4","executionInfo":{"status":"ok","timestamp":1587835469042,"user_tz":-330,"elapsed":108227,"user":{"displayName":"Edgar Monis","photoUrl":"","userId":"18244376460613966063"}},"colab":{"base_uri":"https://localhost:8080/","height":480,"referenced_widgets":["29f9159510454568aebe8f65ff815e6a","5c668e33f9884e2dbc92b3441c1378a6","dbe06afa99a340319bf353a7cd107a56","250f15549ded495cbc8c43638fa0d0c1","ccda3df2e5f6493b9a97135df5e9236c","5c4da7f4ef8f404cb96a5787e6a73efc","bf6ee12da564499e9d4f5dcffb62960d","cedac71061294662aabf8b4ca5d23be8"]}},"source":["# Sentence Handler\n","from spacy.lang.en import English\n","from typing import List\n","\n","# Cluster Features\n","import numpy as np\n","from numpy import ndarray\n","from sklearn.cluster import KMeans\n","from sklearn.mixture import GaussianMixture\n","from sklearn.decomposition import PCA\n","from typing import List\n","\n","# Run model\n","import torch\n","\n","class SentenceHandler(object):\n","\n","\n","    def __init__(self, language=English):\n","        self.nlp = language()\n","        self.nlp.add_pipe(self.nlp.create_pipe('sentencizer'))\n","\n","    def process(self, body: str, min_length: int = 40, max_length: int = 600) -> List[str]:\n","        \"\"\"\n","        Processes the content sentences.\n","        :param body: The raw string body to process\n","        :param min_length: Minimum length that the sentences must be\n","        :param max_length: Max length that the sentences mus fall under\n","        :return: Returns a list of sentences.\n","        \"\"\"\n","        doc = self.nlp(body)\n","        return [c.string.strip() for c in doc.sents if max_length > len(c.string.strip()) > min_length]\n","\n","    def __call__(self, body: str, min_length: int = 40, max_length: int = 600) -> List[str]:\n","        return self.process(body, min_length, max_length)\n","\n","class ClusterFeatures(object):\n","    \"\"\"\n","    Basic handling of clustering features.\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        features: ndarray,\n","        algorithm: str = 'kmeans',\n","        pca_k: int = None,\n","        random_state: int = 12345\n","    ):\n","        \"\"\"\n","        :param features: the embedding matrix created by bert parent\n","        :param algorithm: Which clustering algorithm to use\n","        :param pca_k: If you want the features to be ran through pca, this is the components number\n","        :param random_state: Random state\n","        \"\"\"\n","        self.features = features\n","\n","        self.algorithm = algorithm\n","        self.random_state = random_state\n","        np.random.seed(random_state)\n","\n","    def __get_model(self, k: int):\n","        \"\"\"\n","        Retrieve clustering model\n","        :param k: amount of clusters\n","        :return: Clustering model\n","        \"\"\"\n","\n","        if self.algorithm == 'gmm':\n","            return GaussianMixture(n_components=k, random_state=self.random_state)\n","\n","        return KMeans(n_clusters=k, random_state=self.random_state)\n","\n","    def __get_centroids(self, model):\n","        \"\"\"\n","        Retrieve centroids of model\n","        :param model: Clustering model\n","        :return: Centroids\n","        \"\"\"\n","\n","        if self.algorithm == 'gmm':\n","            return model.means_\n","        return model.cluster_centers_\n","\n","    def __find_closest_args(self, centroids: np.ndarray):\n","        \"\"\"\n","        Find the closest arguments to centroid\n","        :param centroids: Centroids to find closest\n","        :return: Closest arguments\n","        \"\"\"\n","\n","        centroid_min = 1e10\n","        cur_arg = -1\n","        args = {}\n","        used_idx = []\n","\n","        for j, centroid in enumerate(centroids):\n","\n","            for i, feature in enumerate(self.features):\n","                value = np.linalg.norm(feature - centroid)\n","\n","                if value < centroid_min and i not in used_idx:\n","                    cur_arg = i\n","                    centroid_min = value\n","\n","            used_idx.append(cur_arg)\n","            args[j] = cur_arg\n","            centroid_min = 1e10\n","            cur_arg = -1\n","\n","        return args\n","\n","    def cluster(self, ratio: float = 0.1) -> List[int]:\n","        \"\"\"\n","        Clusters sentences based on the ratio\n","        :param ratio: Ratio to use for clustering\n","        :return: Sentences index that qualify for summary\n","        \"\"\"\n","\n","        k = 1 if ratio * len(self.features) < 1 else int(len(self.features) * ratio)\n","        model = self.__get_model(k).fit(self.features)\n","        centroids = self.__get_centroids(model)\n","        cluster_args = self.__find_closest_args(centroids)\n","        sorted_values = sorted(cluster_args.values())\n","        return sorted_values\n","\n","    def __call__(self, ratio: float = 0.1) -> List[int]:\n","        return self.cluster(ratio)\n","\n","def Summary(body,ratio=0.3):\n","    # Sentences are cleaned up by the sentence handler\n","    nlp_sentence_handler = SentenceHandler()\n","    sentences = nlp_sentence_handler(body, min_length=2, max_length=600)\n","    \n","    # The Sentences are tokenised\n","    nlp_tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n","    tokenised_text_list = [ nlp_tokenizer.tokenize(t)              for t in sentences ]\n","    \n","    # Indexes of tokens are obtained\n","    indexed_tokens_list = [ nlp_tokenizer.convert_tokens_to_ids(t) for t in tokenised_text_list ]\n","\n","    # Init the nlp model\n","    nlp_model = BertModel.from_pretrained('bert-large-uncased', output_hidden_states=True).to('cuda:0')\n","    nlp_model.eval()\n","\n","    # The sentences are passes through the model one at a time because of variable length\n","    sentence_embeddings = []\n","\n","    for x in indexed_tokens_list:\n","        sent_tensor = torch.tensor([x]).to('cuda:0')\n","        pooled, hidden_states = nlp_model(sent_tensor)[-2:]\n","        pooled = hidden_states[-2].mean(dim=1)\n","        sentence_embeddings.append(pooled)\n","\n","    hidden_features = np.array([np.squeeze(x.detach().cpu().numpy()) for x in sentence_embeddings])\n","\n","    hidden_args = ClusterFeatures(hidden_features, algorithm='kmeans', random_state=12345).cluster(ratio)\n","\n","    if hidden_args[0] != 0:\n","        hidden_args.insert(0,0)\n","\n","    result = [sentences[j] for j in hidden_args]\n","\n","    result = ''.join(result).replace('.','.\\n')\n","\n","    print(result)\n","\n","Summary(game_of_thrones)"],"execution_count":3,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"29f9159510454568aebe8f65ff815e6a","version_minor":0,"version_major":2},"text/plain":["HBox(children=(IntProgress(value=0, description='Downloading', max=231508, style=ProgressStyle(description_wid…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","A Game of Thrones takes place over the course of one year on or near the fictional continent of Westeros.\n","Just after the royal party arrives, Ned’s wife, Catelyn, receives a message claiming that the Lannister family was responsible for the death of the former Hand.\n","Tyrion Lannister, a little person who is brother to Cersei and Jaime, travels with Jon to the Wall to see the massive structure.\n","A knight named Ser Jorah Mormont, exiled by Ned Stark, pledges he will help.\n","On the trip south to the capital, called King’s Landing, Robert’s and Cersei’s son, Joffrey, and Ned’s daughter Sansa, who everyone presumes will be married one day, go for a walk.\n","Bran finally wakes from his coma, but he doesn’t remember how he fell.\n","He knows it has to do with something the Hand learned about King Robert’s children.\n","Through a spy, Robert learns that Daenerys Targaryen is pregnant.\n","That night, Jaime and his men confront Ned about Tyrion’s capture.\n","Jaime kills Ned’s men and Ned breaks his leg while fighting.\n","The following day, Robert reinstates Ned as hand.\n","Tyrion denies the accusations and demands a trial by combat.\n","Tyrion’s mercenary wins.\n","In the east, as Khal Drogo and his Dothraki followers head back to Vaes Dothrak, their capital, Viserys becomes increasingly angry that Drogo has not provided him with an army with which to wage war on Westeros.\n","Ned asks Littlefinger’s help to install the true heir, Robert’s brother Stannis, as the king, and Littlefinger agrees.\n","Meanwhile, north of the Wall, Jon and other men have discovered two strange dead bodies.\n","Shrewdly outmaneuvering Tywin, Robb manages to defeat a portion of the Lannister army and capture Jaime.\n","In the hopes that he can prevent his daughters being harmed, Ned confesses publicly to treason, and Joffrey has him executed while Sansa watches.\n","In the east, Drogo suffers a wound while raiding a village.\n","Shortly thereafter Daenerys goes into labor.\n","When she wakes many days later, her child is dead and Drogo is alive but brain dead.\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"28Bmps_tE90e","colab_type":"text"},"source":["# The Bert project structure is as follows:\n","\n","## Config class\n","\n","## Use config class to set number of layers\n","\n","## class for the different layers used\n","\n","### The layers used are :\n","\n","*   Self Attention\n","*   Feed forward\n","*   Position encoding embedding\n","*   Custom Activation functions like gelu etc\n","*   Embeddings\n","\n"]},{"cell_type":"code","metadata":{"id":"RoZeM4W0FC6y","colab_type":"code","colab":{}},"source":["import torch\n","from torch import nn\n","import torch.nn.functional as F\n","from transformers import *\n","from torch import Tensor, device, dtype, nn\n","import math\n","\n","def mish(x):\n","    return x * torch.tanh(nn.functional.softplus(x))\n","\n","def swish(x):\n","    return x * torch.sigmoid(x)\n","\n","def gelu_new(x):\n","    \"\"\" Implementation of the gelu activation function currently in Google Bert repo (identical to OpenAI GPT).\n","        Also see https://arxiv.org/abs/1606.08415\n","    \"\"\"\n","    return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n","\n","ACT2FN = {\"gelu\": F.gelu, \"relu\": torch.nn.functional.relu, \"swish\": swish, \"gelu_new\": gelu_new, \"mish\": mish}\n","\n","# All relevant code\n","class EdBertEmbeddings(nn.Module):\n","    \"\"\"Construct the embeddings from word, position and token_type embeddings.\n","    \"\"\"\n","\n","    def __init__(self, config):\n","        super().__init__()\n","        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)\n","        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n","        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n","\n","        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n","        # any TensorFlow checkpoint file\n","        self.LayerNorm = torch.nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n","        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n","\n","    def forward(self, input_ids=None, token_type_ids=None, position_ids=None, inputs_embeds=None):\n","        input_shape = input_ids.size()\n","        seq_length = input_shape[1]\n","        device = input_ids.device\n","\n","        position_ids = torch.arange(seq_length, dtype=torch.long, device=device)\n","        position_ids = position_ids.unsqueeze(0).expand(input_shape)\n","\n","        token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n","        inputs_embeds = self.word_embeddings(input_ids)\n","\n","        position_embeddings = self.position_embeddings(position_ids)\n","        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n","\n","        embeddings = inputs_embeds + position_embeddings + token_type_embeddings\n","        embeddings = self.LayerNorm(embeddings)\n","        embeddings = self.dropout(embeddings)\n","\n","        return embeddings\n","\n","class EdBertSelfAttention(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n","            raise ValueError(\n","                \"The hidden size (%d) is not a multiple of the number of attention \"\n","                \"heads (%d)\" % (config.hidden_size, config.num_attention_heads)\n","            )\n","        self.output_attentions = config.output_attentions\n","\n","        self.num_attention_heads = config.num_attention_heads\n","        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n","        self.all_head_size = self.num_attention_heads * self.attention_head_size\n","\n","        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n","        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n","        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n","\n","        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n","\n","    def transpose_for_scores(self, x):\n","        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n","        x = x.view(*new_x_shape)\n","        return x.permute(0, 2, 1, 3)\n","\n","    def forward(\n","        self,\n","        hidden_states,\n","        attention_mask=None,\n","        head_mask=None,\n","        encoder_hidden_states=None,\n","        encoder_attention_mask=None,\n","    ):\n","        mixed_query_layer = self.query(hidden_states)\n","\n","        # If this is instantiated as a cross-attention module, the keys\n","        # and values come from an encoder; the attention mask needs to be\n","        # such that the encoder's padding tokens are not attended to.\n","        if encoder_hidden_states is not None:\n","            mixed_key_layer = self.key(encoder_hidden_states)\n","            mixed_value_layer = self.value(encoder_hidden_states)\n","            attention_mask = encoder_attention_mask\n","        else:\n","            mixed_key_layer = self.key(hidden_states)\n","            mixed_value_layer = self.value(hidden_states)\n","\n","        query_layer = self.transpose_for_scores(mixed_query_layer)\n","        key_layer = self.transpose_for_scores(mixed_key_layer)\n","        value_layer = self.transpose_for_scores(mixed_value_layer)\n","\n","        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n","        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n","        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n","        if attention_mask is not None:\n","            # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n","            attention_scores = attention_scores + attention_mask\n","\n","        # Normalize the attention scores to probabilities.\n","        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n","\n","        # This is actually dropping out entire tokens to attend to, which might\n","        # seem a bit unusual, but is taken from the original Transformer paper.\n","        attention_probs = self.dropout(attention_probs)\n","\n","        # Mask heads if we want to\n","        if head_mask is not None:\n","            attention_probs = attention_probs * head_mask\n","\n","        context_layer = torch.matmul(attention_probs, value_layer)\n","\n","        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n","        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n","        context_layer = context_layer.view(*new_context_layer_shape)\n","\n","        outputs = (context_layer, attention_probs) if self.output_attentions else (context_layer,)\n","        return outputs\n","\n","class EdBertSelfOutput(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n","        self.LayerNorm = torch.nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n","        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n","\n","    def forward(self, hidden_states, input_tensor):\n","        hidden_states = self.dense(hidden_states)\n","        hidden_states = self.dropout(hidden_states)\n","        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n","        return hidden_states\n","\n","class EdBertAttention(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        self.self = EdBertSelfAttention(config)\n","        self.output = EdBertSelfOutput(config)\n","        self.pruned_heads = set()\n","\n","    def prune_heads(self, heads):\n","        if len(heads) == 0:\n","            return\n","        mask = torch.ones(self.self.num_attention_heads, self.self.attention_head_size)\n","        heads = set(heads) - self.pruned_heads  # Convert to set and remove already pruned heads\n","        for head in heads:\n","            # Compute how many pruned heads are before the head and move the index accordingly\n","            head = head - sum(1 if h < head else 0 for h in self.pruned_heads)\n","            mask[head] = 0\n","        mask = mask.view(-1).contiguous().eq(1)\n","        index = torch.arange(len(mask))[mask].long()\n","\n","        # Prune linear layers\n","        self.self.query = prune_linear_layer(self.self.query, index)\n","        self.self.key = prune_linear_layer(self.self.key, index)\n","        self.self.value = prune_linear_layer(self.self.value, index)\n","        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n","\n","        # Update hyper params and store pruned heads\n","        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n","        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n","        self.pruned_heads = self.pruned_heads.union(heads)\n","\n","    def forward(\n","        self,\n","        hidden_states,\n","        attention_mask=None,\n","        head_mask=None,\n","        encoder_hidden_states=None,\n","        encoder_attention_mask=None,\n","    ):\n","        self_outputs = self.self(\n","            hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask\n","        )\n","        attention_output = self.output(self_outputs[0], hidden_states)\n","        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n","        return outputs\n","\n","class EdBertIntermediate(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n","        if isinstance(config.hidden_act, str):\n","            self.intermediate_act_fn = ACT2FN[config.hidden_act]\n","        else:\n","            self.intermediate_act_fn = config.hidden_act\n","\n","    def forward(self, hidden_states):\n","        hidden_states = self.dense(hidden_states)\n","        hidden_states = self.intermediate_act_fn(hidden_states)\n","        return hidden_states\n","\n","class EdBertOutput(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n","        self.LayerNorm = torch.nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n","        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n","\n","    def forward(self, hidden_states, input_tensor):\n","        hidden_states = self.dense(hidden_states)\n","        hidden_states = self.dropout(hidden_states)\n","        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n","        return hidden_states\n","\n","class EdBertLayer(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        self.attention = EdBertAttention(config)\n","        self.is_decoder = config.is_decoder\n","        if self.is_decoder:\n","            self.crossattention = EdBertAttention(config)\n","        self.intermediate = EdBertIntermediate(config)\n","        self.output = EdBertOutput(config)\n","\n","    def forward(\n","        self,\n","        hidden_states,\n","        attention_mask=None,\n","        head_mask=None,\n","        encoder_hidden_states=None,\n","        encoder_attention_mask=None,\n","    ):\n","        self_attention_outputs = self.attention(hidden_states, attention_mask, head_mask)\n","        attention_output = self_attention_outputs[0]\n","        outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n","\n","        if self.is_decoder and encoder_hidden_states is not None:\n","            cross_attention_outputs = self.crossattention(\n","                attention_output, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask\n","            )\n","            attention_output = cross_attention_outputs[0]\n","            outputs = outputs + cross_attention_outputs[1:]  # add cross attentions if we output attention weights\n","\n","        intermediate_output = self.intermediate(attention_output)\n","        layer_output = self.output(intermediate_output, attention_output)\n","        outputs = (layer_output,) + outputs\n","        return outputs\n","\n","class EdBertEncoder(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        self.output_attentions = config.output_attentions\n","        self.output_hidden_states = config.output_hidden_states\n","        self.layer = nn.ModuleList([EdBertLayer(config) for _ in range(config.num_hidden_layers)])\n","\n","    def forward(\n","        self,\n","        hidden_states,\n","        attention_mask=None,\n","        head_mask=None,\n","        encoder_hidden_states=None,\n","        encoder_attention_mask=None,\n","    ):\n","        all_hidden_states = ()\n","        all_attentions = ()\n","        for i, layer_module in enumerate(self.layer):\n","            if self.output_hidden_states:\n","                all_hidden_states = all_hidden_states + (hidden_states,)\n","\n","            layer_outputs = layer_module(\n","                hidden_states, attention_mask, head_mask[i], encoder_hidden_states, encoder_attention_mask\n","            )\n","            hidden_states = layer_outputs[0]\n","\n","            if self.output_attentions:\n","                all_attentions = all_attentions + (layer_outputs[1],)\n","\n","        # Add last layer\n","        if self.output_hidden_states:\n","            all_hidden_states = all_hidden_states + (hidden_states,)\n","\n","        outputs = (hidden_states,)\n","        if self.output_hidden_states:\n","            outputs = outputs + (all_hidden_states,)\n","        if self.output_attentions:\n","            outputs = outputs + (all_attentions,)\n","        return outputs  # last-layer hidden state, (all hidden states), (all attentions)\n","\n","class EdBertPooler(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n","        self.activation = nn.Tanh()\n","\n","    def forward(self, hidden_states):\n","        # We \"pool\" the model by simply taking the hidden state corresponding\n","        # to the first token.\n","        first_token_tensor = hidden_states[:, 0]\n","        pooled_output = self.dense(first_token_tensor)\n","        pooled_output = self.activation(pooled_output)\n","        return pooled_output\n","\n","class EdBertPreTrainedModel(PreTrainedModel):\n","    \"\"\" An abstract class to handle weights initialization and\n","        a simple interface for downloading and loading pretrained models.\n","    \"\"\"\n","\n","    config_class = BertConfig\n","    pretrained_model_archive_map = BERT_PRETRAINED_MODEL_ARCHIVE_MAP\n","    load_tf_weights = load_tf_weights_in_bert\n","    base_model_prefix = \"bert\"\n","\n","    def _init_weights(self, module):\n","        \"\"\" Initialize the weights \"\"\"\n","        if isinstance(module, (nn.Linear, nn.Embedding)):\n","            # Slightly different from the TF version which uses truncated_normal for initialization\n","            # cf https://github.com/pytorch/pytorch/pull/5617\n","            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n","        elif isinstance(module, torch.nn.LayerNorm):\n","            module.bias.data.zero_()\n","            module.weight.data.fill_(1.0)\n","        if isinstance(module, nn.Linear) and module.bias is not None:\n","            module.bias.data.zero_()\n","\n","class EdBertModel(EdBertPreTrainedModel):\n","    \"\"\"\n","    The model can behave as an encoder (with only self-attention) as well\n","    as a decoder, in which case a layer of cross-attention is added between\n","    the self-attention layers, following the architecture described in `Attention is all you need`_ by Ashish Vaswani,\n","    Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser and Illia Polosukhin.\n","    To behave as an decoder the model needs to be initialized with the\n","    :obj:`is_decoder` argument of the configuration set to :obj:`True`; an\n","    :obj:`encoder_hidden_states` is expected as an input to the forward pass.\n","    .. _`Attention is all you need`:\n","        https://arxiv.org/abs/1706.03762\n","    \"\"\"\n","\n","    def __init__(self, config):\n","        super().__init__(config)\n","        self.config = config\n","\n","        # self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)\n","        # self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n","        # self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n","        # self.LayerNorm = torch.nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n","        # self.dropout = nn.Dropout(config.hidden_dropout_prob)\n","\n","\n","        self.embeddings = EdBertEmbeddings(config)\n","        self.encoder = EdBertEncoder(config)\n","        self.pooler = EdBertPooler(config)\n","\n","        self.init_weights()\n","\n","    def get_input_embeddings(self):\n","        return self.embeddings.word_embeddings\n","\n","    def set_input_embeddings(self, value):\n","        self.embeddings.word_embeddings = value\n","\n","    def _prune_heads(self, heads_to_prune):\n","        \"\"\" Prunes heads of the model.\n","            heads_to_prune: dict of {layer_num: list of heads to prune in this layer}\n","            See base class PreTrainedModel\n","        \"\"\"\n","        for layer, heads in heads_to_prune.items():\n","            self.encoder.layer[layer].attention.prune_heads(heads)\n","\n","    def invert_attention_mask(self, encoder_attention_mask: Tensor) -> Tensor:\n","        \"\"\"type: torch.Tensor -> torch.Tensor\"\"\"\n","        if encoder_attention_mask.dim() == 3:\n","            encoder_extended_attention_mask = encoder_attention_mask[:, None, :, :]\n","        if encoder_attention_mask.dim() == 2:\n","            encoder_extended_attention_mask = encoder_attention_mask[:, None, None, :]\n","        # T5 has a mask that can compare sequence ids, we can simulate this here with this transposition\n","        # Cf. https://github.com/tensorflow/mesh/blob/8d2465e9bc93129b913b5ccc6a59aa97abd96ec6/mesh_tensorflow\n","        # /transformer/transformer_layers.py#L270\n","        # encoder_extended_attention_mask = (encoder_extended_attention_mask ==\n","        # encoder_extended_attention_mask.transpose(-1, -2))\n","        encoder_extended_attention_mask = encoder_extended_attention_mask.to(dtype=self.dtype)  # fp16 compatibility\n","        encoder_extended_attention_mask = (1.0 - encoder_extended_attention_mask) * -1e9\n","        return encoder_extended_attention_mask\n","\n","    def get_head_mask(self, head_mask, num_hidden_layers):\n","        \"\"\"\n","        # Prepare head mask if needed\n","        # 1.0 in head_mask indicate we keep the head\n","        attention_probs has shape bsz x n_heads x N x N\n","        Arguments:\n","            head_mask: torch.Tensor or None: has shape [num_heads] or [num_hidden_layers x num_heads]\n","            num_hidden_layers: int\n","        Returns:\n","             Tensor of shape shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n","             or list with [None] for each layer\n","        \"\"\"\n","        if head_mask is not None:\n","            head_mask = self._convert_head_mask_to_5d(head_mask, num_hidden_layers)\n","        else:\n","            head_mask = [None] * num_hidden_layers\n","\n","        return head_mask\n","\n","    def get_extended_attention_mask(self, attention_mask: Tensor, input_shape: tuple, device: device):\n","        \"\"\"Makes broadcastable attention mask and causal mask so that future and maked tokens are ignored.\n","        Arguments:\n","            attention_mask: torch.Tensor with 1 indicating tokens to ATTEND to\n","            input_shape: tuple, shape of input_ids\n","            device: torch.Device, usually self.device\n","        Returns:\n","            torch.Tensor with dtype of attention_mask.dtype\n","        \"\"\"\n","        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n","        # ourselves in which case we just need to make it broadcastable to all heads.\n","        if attention_mask.dim() == 3:\n","            extended_attention_mask = attention_mask[:, None, :, :]\n","        elif attention_mask.dim() == 2:\n","            # Provided a padding mask of dimensions [batch_size, seq_length]\n","            # - if the model is a decoder, apply a causal mask in addition to the padding mask\n","            # - if the model is an encoder, make the mask broadcastable to [batch_size, num_heads, seq_length, seq_length]\n","            if self.config.is_decoder:\n","                batch_size, seq_length = input_shape\n","                seq_ids = torch.arange(seq_length, device=device)\n","                causal_mask = seq_ids[None, None, :].repeat(batch_size, seq_length, 1) <= seq_ids[None, :, None]\n","                # causal and attention masks must have same type with pytorch version < 1.3\n","                causal_mask = causal_mask.to(attention_mask.dtype)\n","                extended_attention_mask = causal_mask[:, None, :, :] * attention_mask[:, None, None, :]\n","            else:\n","                extended_attention_mask = attention_mask[:, None, None, :]\n","        else:\n","            raise ValueError(\n","                \"Wrong shape for input_ids (shape {}) or attention_mask (shape {})\".format(\n","                    input_shape, attention_mask.shape\n","                )\n","            )\n","\n","        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n","        # masked positions, this operation will create a tensor which is 0.0 for\n","        # positions we want to attend and -10000.0 for masked positions.\n","        # Since we are adding it to the raw scores before the softmax, this is\n","        # effectively the same as removing these entirely.\n","        # extended_attention_mask = extended_attention_mask.to(dtype=self.dtype)  # fp16 compatibility\n","        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n","        return extended_attention_mask\n","\n","    def forward(\n","        self,\n","        input_ids=None,\n","        attention_mask=None,\n","        token_type_ids=None,\n","        position_ids=None,\n","        head_mask=None,\n","        inputs_embeds=None,\n","        encoder_hidden_states=None,\n","        encoder_attention_mask=None,\n","    ):\n","\n","        input_shape = input_ids.size()\n","        \n","        device = input_ids.device\n","\n","        attention_mask = torch.ones(input_shape, device=device)\n","        \n","        token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n","\n","        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n","        # ourselves in which case we just need to make it broadcastable to all heads.\n","\n","        extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(\n","            attention_mask, input_shape, device\n","        )\n","\n","        encoder_extended_attention_mask = None\n","\n","        # Prepare head mask if needed\n","        # 1.0 in head_mask indicate we keep the head\n","        # attention_probs has shape bsz x n_heads x N x N\n","        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n","        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n","        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n","\n","        embedding_output = self.embeddings(\n","            input_ids=input_ids, position_ids=position_ids, token_type_ids=token_type_ids, inputs_embeds=inputs_embeds\n","        )\n","\n","        encoder_outputs = self.encoder(\n","            embedding_output,\n","            attention_mask=extended_attention_mask,\n","            head_mask=head_mask,\n","            encoder_hidden_states=encoder_hidden_states,\n","            encoder_attention_mask=encoder_extended_attention_mask,\n","        )\n","\n","        sequence_output = encoder_outputs[0]\n","        pooled_output = self.pooler(sequence_output)\n","\n","        outputs = (sequence_output, pooled_output,) + encoder_outputs[\n","            1:\n","        ]  # add hidden_states and attentions if they are here\n","        return outputs  # sequence_output, pooled_output, (hidden_states), (attentions)\n","\n","model = EdBertModel.from_pretrained('bert-large-uncased', output_hidden_states=True).to('cuda:0').eval()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"78YugtbgQ95M","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":85},"outputId":"25e1a6bf-1b18-4fde-eacb-52b3b5e0df36","executionInfo":{"status":"ok","timestamp":1587836222832,"user_tz":-330,"elapsed":1469,"user":{"displayName":"Edgar Monis","photoUrl":"","userId":"18244376460613966063"}}},"source":["sent_tensor = torch.tensor([indexed_tokens_list[0]]).to('cuda:0')\n","print(nlp_model(sent_tensor)[-2:][0])\n","print(model(sent_tensor)[-2:][0])"],"execution_count":20,"outputs":[{"output_type":"stream","text":["tensor([[ 0.9855,  0.5987,  0.2476,  ..., -0.9527,  0.5409, -0.7655]],\n","       device='cuda:0', grad_fn=<TanhBackward>)\n","tensor([[ 0.9855,  0.5987,  0.2476,  ..., -0.9527,  0.5409, -0.7655]],\n","       device='cuda:0', grad_fn=<TanhBackward>)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1oDvD-ikah5X","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":170},"outputId":"49696274-3974-49f0-91c9-a133b54e9a85","executionInfo":{"status":"ok","timestamp":1587836283789,"user_tz":-330,"elapsed":1139,"user":{"displayName":"Edgar Monis","photoUrl":"","userId":"18244376460613966063"}}},"source":["nlp_model.embeddings.word_embeddings"],"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Parameter containing:\n","tensor([[-0.0419,  0.0022, -0.0314,  ..., -0.0146, -0.0358, -0.0929],\n","        [-0.0434, -0.0115, -0.0065,  ..., -0.0273, -0.0310, -0.1084],\n","        [-0.0622, -0.0114, -0.0073,  ..., -0.0247, -0.0334, -0.0915],\n","        ...,\n","        [ 0.0239, -0.0083, -0.0061,  ...,  0.0176,  0.0214, -0.0763],\n","        [-0.0468, -0.0018,  0.0020,  ..., -0.0173, -0.0243, -0.1124],\n","        [-0.0185, -0.0369, -0.0623,  ..., -0.0054, -0.0412, -0.0847]],\n","       device='cuda:0', requires_grad=True)"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"code","metadata":{"id":"ixRGYFqIRnaz","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":391},"outputId":"282e5916-0f03-45e5-848f-7a4b6c0a592e","executionInfo":{"status":"ok","timestamp":1587835594687,"user_tz":-330,"elapsed":23661,"user":{"displayName":"Edgar Monis","photoUrl":"","userId":"18244376460613966063"}}},"source":["# nlp_sentence_handler = SentenceHandler()\n","# sentences = nlp_sentence_handler(game_of_thrones, min_length=2, max_length=600)\n","\n","# # The Sentences are tokenised\n","# nlp_tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n","# tokenised_text_list = [ nlp_tokenizer.tokenize(t)              for t in sentences ]\n","\n","# # Indexes of tokens are obtained\n","# indexed_tokens_list = [ nlp_tokenizer.convert_tokens_to_ids(t) for t in tokenised_text_list ]\n","\n","# # Init the nlp model\n","# nlp_model = BertModel.from_pretrained('bert-large-uncased', output_hidden_states=True).to('cuda:0')\n","# nlp_model.eval()\n","\n","# sentence_embeddings = []\n","# ed_sente_embeddings = []\n","\n","# for x in indexed_tokens_list:\n","#     sent_tensor = torch.tensor([x]).to('cuda:0')\n","#     pooled, hidden_states = nlp_model(sent_tensor)[-2:]\n","#     pooled = hidden_states[-2].mean(dim=1)\n","#     sentence_embeddings.append(pooled)\n","\n","# for x in indexed_tokens_list:\n","#     sent_tensor = torch.tensor([x]).to('cuda:0')\n","#     pooled, hidden_states = model(sent_tensor)[-2:]\n","#     pooled = hidden_states[-2].mean(dim=1)\n","#     ed_sente_embeddings.append(pooled)\n","# print(sentence_embeddings[:10])\n","# print(ed_sente_embeddings[:10])"],"execution_count":9,"outputs":[{"output_type":"stream","text":["[tensor([[-0.4748,  0.0979, -0.6836,  ...,  0.3399,  0.9363,  0.6940]],\n","       device='cuda:0', grad_fn=<MeanBackward1>), tensor([[-0.3602, -0.2892, -0.1944,  ..., -0.2201,  0.5856,  0.5485]],\n","       device='cuda:0', grad_fn=<MeanBackward1>), tensor([[-0.4756, -0.2973, -0.6364,  ...,  0.0732,  1.0698,  0.2417]],\n","       device='cuda:0', grad_fn=<MeanBackward1>), tensor([[-0.3085, -0.4276,  0.0906,  ..., -0.1989,  0.6127, -0.2816]],\n","       device='cuda:0', grad_fn=<MeanBackward1>), tensor([[-0.3524, -0.2862, -0.0366,  ..., -0.3369,  0.4495,  0.1495]],\n","       device='cuda:0', grad_fn=<MeanBackward1>), tensor([[-0.1603, -0.4672,  0.1032,  ..., -0.1580,  0.4831, -0.3107]],\n","       device='cuda:0', grad_fn=<MeanBackward1>), tensor([[ 0.1564, -0.1959, -0.0484,  ..., -0.1220,  0.8050,  0.0360]],\n","       device='cuda:0', grad_fn=<MeanBackward1>), tensor([[-0.6288, -0.8354, -0.7515,  ...,  0.1547,  1.0431,  0.0763]],\n","       device='cuda:0', grad_fn=<MeanBackward1>), tensor([[-0.3943, -0.3572,  0.0406,  ..., -0.3017,  0.3627, -0.3198]],\n","       device='cuda:0', grad_fn=<MeanBackward1>), tensor([[-0.1631, -0.2317, -0.2952,  ..., -0.0763,  0.3909, -0.0611]],\n","       device='cuda:0', grad_fn=<MeanBackward1>)]\n","[tensor([[-0.4748,  0.0979, -0.6836,  ...,  0.3399,  0.9363,  0.6940]],\n","       device='cuda:0', grad_fn=<MeanBackward1>), tensor([[-0.3602, -0.2892, -0.1944,  ..., -0.2201,  0.5856,  0.5485]],\n","       device='cuda:0', grad_fn=<MeanBackward1>), tensor([[-0.4756, -0.2973, -0.6364,  ...,  0.0732,  1.0698,  0.2417]],\n","       device='cuda:0', grad_fn=<MeanBackward1>), tensor([[-0.3085, -0.4276,  0.0906,  ..., -0.1990,  0.6127, -0.2816]],\n","       device='cuda:0', grad_fn=<MeanBackward1>), tensor([[-0.3524, -0.2862, -0.0366,  ..., -0.3369,  0.4495,  0.1495]],\n","       device='cuda:0', grad_fn=<MeanBackward1>), tensor([[-0.1603, -0.4672,  0.1032,  ..., -0.1580,  0.4831, -0.3107]],\n","       device='cuda:0', grad_fn=<MeanBackward1>), tensor([[ 0.1564, -0.1959, -0.0484,  ..., -0.1220,  0.8050,  0.0360]],\n","       device='cuda:0', grad_fn=<MeanBackward1>), tensor([[-0.6288, -0.8354, -0.7515,  ...,  0.1547,  1.0431,  0.0763]],\n","       device='cuda:0', grad_fn=<MeanBackward1>), tensor([[-0.3943, -0.3572,  0.0406,  ..., -0.3017,  0.3627, -0.3198]],\n","       device='cuda:0', grad_fn=<MeanBackward1>), tensor([[-0.1631, -0.2317, -0.2952,  ..., -0.0763,  0.3909, -0.0611]],\n","       device='cuda:0', grad_fn=<MeanBackward1>)]\n"],"name":"stdout"}]}]}